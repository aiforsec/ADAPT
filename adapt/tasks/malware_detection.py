from adapt.tasks.base_task import BaseTask
from adapt.models import MODEL_DICT
from adapt.datasets.dataset import MalwareDataset
from adapt.tasks.config import get_config
from adapt.utils import seed_everything
from adapt.utils.metrics import calculate_monthwise_metrics

import os
import numpy as np
from sklearn.preprocessing import StandardScaler
import pickle
from collections import defaultdict
import argparse


class MalwareDetection(BaseTask):
    def __init__(self, cfg):
        """
        Binary malware detection task

        Args:
            cfg: Configuration dictionary for the task.
        """
        super().__init__(cfg)
        seed_everything(cfg.EXPERIMENT.SEED)

        self.train_dataset, self.val_dataset, self.test_dataset = self.load_data()
        self.standard_scaler = None

        self.model = self.build_model(cfg)
        if cfg.EXPERIMENT.VALIDATION_MODE:
            # random seed is used for hyperparam generation
            self.out_dir = os.path.join(cfg.EXPERIMENT.OUT_DIR, cfg.EXPERIMENT.TASK, cfg.DATASET.NAME + "_" +
                                        cfg.DATASET.DUPLICATES, cfg.EXPERIMENT.MODEL_NAME, str(cfg.EXPERIMENT.SEED))
        else:
            self.out_dir = os.path.join(cfg.EXPERIMENT.OUT_DIR, cfg.EXPERIMENT.TASK, cfg.DATASET.NAME + "_" +
                                        cfg.DATASET.DUPLICATES, cfg.EXPERIMENT.MODEL_NAME)
        os.makedirs(self.out_dir, exist_ok=True)
        self.out_dir = str(self.out_dir)

        self.write_cfg()
        self.write_params()

    def write_cfg(self):
        with open(os.path.join(self.out_dir, 'cfg.txt'), 'w') as f:
            f.write(self.cfg.dump())

    def write_params(self):
        with open(os.path.join(self.out_dir, 'model_params.pkl'), 'wb') as f:
            # print(self.model.params)
            pickle.dump(self.model.params, f)

    def load_data(self):
        """
        Loads and preprocesses the data for the task.
        """
        # load train/val/test datasets from config
        cfg = self.cfg
        train_dataset = MalwareDataset(cfg.DATASET.DATA_DIR, split='train', cfg=cfg.DATASET)
        val_dataset = MalwareDataset(cfg.DATASET.DATA_DIR, split='val', cfg=cfg.DATASET)
        test_dataset = MalwareDataset(cfg.DATASET.DATA_DIR, split='test', cfg=cfg.DATASET)
        return train_dataset, val_dataset, test_dataset

    def build_model(self, cfg):
        if not cfg.EXPERIMENT.VALIDATION_MODE and cfg.EXPERIMENT.PARAMS is not None:
            params = MODEL_DICT[cfg.EXPERIMENT.MODEL_NAME].load_parameters(cfg.EXPERIMENT.PARAMS)
        elif cfg.EXPERIMENT.SEED == 0:
            params = MODEL_DICT[cfg.EXPERIMENT.MODEL_NAME].default_parameters()
        else:
            params = MODEL_DICT[cfg.EXPERIMENT.MODEL_NAME].get_random_parameters(cfg.EXPERIMENT.SEED)

        print(params)
        return MODEL_DICT[cfg.EXPERIMENT.MODEL_NAME](params, cfg)

    def train(self):
        """
        Trains the given model on the training data.
        """
        if self.cfg.EXPERIMENT.VALIDATION_MODE or self.cfg.EXPERIMENT.MERGE_VAL_DATA_FOR_TEST is False:
            X = self.train_dataset.features
            y = self.train_dataset.binary_labels
            families = self.train_dataset.family_labels
        else:
            X_train = self.train_dataset.features
            y_train = self.train_dataset.binary_labels
            families_train = self.train_dataset.family_labels

            X_val = self.val_dataset.features
            y_val = self.val_dataset.binary_labels
            families_val = self.val_dataset.family_labels

            # Merge training data
            X = np.concatenate((X_train, X_val), axis=0)
            y = np.concatenate((y_train, y_val), axis=0)
            families = np.concatenate((families_train, families_val), axis=0)
        if self.cfg.DATASET.STANDARDIZE:
            self.standard_scaler = StandardScaler()
            X = self.standard_scaler.fit_transform(X)


        import time
        print(X.shape)
        t_start = time.time()
        
        self.model.fit(X, y, families=families)

        print('Training time:', time.time()-t_start)

        
        if not self.cfg.EXPERIMENT.VALIDATION_MODE:
           self.model.save_model(os.path.join(self.out_dir, 'model_'+str(self.cfg.EXPERIMENT.SEED)))

    def write_results_table(self, results_dict, avg_f1, avg_fpr, avg_fnr):
        if self.cfg.EXPERIMENT.VALIDATION_MODE:
            out_file = os.path.join(self.out_dir, 'results.txt')
        else:
            out_file = os.path.join(self.out_dir, 'results_'+str(self.cfg.EXPERIMENT.SEED)+'.txt')
        with open(out_file, 'w') as f:
            # Write header
            f.write("Time\tF1\t\tFPR\t\tFNR\n")

            # Write results for each timestamp
            for timestamp, metrics in results_dict.items():
                f.write(f"{timestamp}\t{metrics['f1']:.4f}\t{metrics['fpr']:.4f}\t{metrics['fnr']:.4f}\n")

            # Write average values
            f.write(f"AVG\t\t{avg_f1:.4f}\t{avg_fpr:.4f}\t{avg_fnr:.4f}\n")

    def evaluate(self):
        """
        Evaluates the trained model on the test data.
        """
        if self.cfg.EXPERIMENT.VALIDATION_MODE:
            X_val = self.val_dataset.features
            y_val = self.val_dataset.binary_labels
            timestamps = self.val_dataset.timestamps
        else:
            X_val = self.test_dataset.features
            y_val = self.test_dataset.binary_labels
            timestamps = self.test_dataset.timestamps

        if self.cfg.DATASET.STANDARDIZE:
            X_val = self.standard_scaler.transform(X_val)

        y_pred = self.model.predict(X_val)
        monthly_metrics, avg_f1, avg_fpr, avg_fnr = calculate_monthwise_metrics(y_val, y_pred, timestamps)
        # print(monthly_metrics)
        self.write_results_table(monthly_metrics, avg_f1, avg_fpr, avg_fnr)
        return monthly_metrics, avg_f1, avg_fpr, avg_fnr

    def save_model(self, model_path):
        """
        Saves the trained model to the specified path.

        Args:
            model_path (str): The path to save the model.
        """
        raise NotImplementedError

    def load_model(self, model_path):
        """
        Loads a trained model from the specified path.

        Args:
            model_path (str): The path to the saved model.

        Returns:
            Any: The loaded model.
        """
        raise NotImplementedError

    def write_aggregate_test_results(self, all_seeds_results):
        # Initialize storage for monthly metrics
        monthly_aggregates = defaultdict(lambda: defaultdict(list))

        # Collect data for each month and each metric
        for seed, monthly_metrics in all_seeds_results.items():
            for month, metrics in monthly_metrics.items():
                for key in ['f1', 'fpr', 'fnr']:
                    monthly_aggregates[month][key].append(metrics[key])

        # Calculate mean and std for each month
        monthly_stats = {}
        for month, metrics in monthly_aggregates.items():
            monthly_stats[month] = {
                'f1_mean': np.mean(metrics['f1']),
                'f1_std': np.std(metrics['f1']),
                'fpr_mean': np.mean(metrics['fpr']),
                'fpr_std': np.std(metrics['fpr']),
                'fnr_mean': np.mean(metrics['fnr']),
                'fnr_std': np.std(metrics['fnr']),
            }

        # Save to a text file
        with open(os.path.join(self.out_dir, 'results.txt'),  'w') as file:
            file.write("Time\tF1\t\tFPR\t\tFNR\n")
            for month in sorted(monthly_stats.keys()):
                stats = monthly_stats[month]
                f1 = f"{stats['f1_mean']:.4f}±{stats['f1_std']:.4f}"
                fpr = f"{stats['fpr_mean']:.4f}±{stats['fpr_std']:.4f}"
                fnr = f"{stats['fnr_mean']:.4f}±{stats['fnr_std']:.4f}"
                file.write(f"{month}\t{f1}\t{fpr}\t{fnr}\n")

    @classmethod
    def run(cls, cfg):
        # only run once during validation mode
        if cfg.EXPERIMENT.VALIDATION_MODE:
            _task = MalwareDetection(cfg)
            _task.train()
            _task.evaluate()
        else:
            # run for different seeds for test mode
            all_seeds_results = {}
            for seed in range(1, cfg.EXPERIMENT.NUM_TEST_RUNS+1):
                cfg.EXPERIMENT.SEED = seed
                _task = MalwareDetection(cfg)
                _task.train()
                monthly_metrics, avg_f1, avg_fpr, avg_fnr  = _task.evaluate()
                all_seeds_results[seed] = monthly_metrics
                all_seeds_results[seed]['AVG'] = {
                    'f1': avg_f1,
                    'fpr': avg_fpr,
                    'fnr': avg_fnr
                }

                if seed == cfg.EXPERIMENT.NUM_TEST_RUNS:
                    _task.write_aggregate_test_results(all_seeds_results)



def run():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, required=True)
    parser.add_argument("--model", type=str, required=True)
    parser.add_argument("--dupes", type=str, default="remove-intra")
    parser.add_argument("--seed", type=int)
    parser.add_argument("--test", action="store_true")
    parser.add_argument("--skip_val", action="store_true")
    parser.add_argument("--params", type=str, default=None)
    args = parser.parse_args()

    _cfg = get_config(args.dataset, args.model)
    _cfg.DATASET.DUPLICATES = args.dupes
    _cfg.EXPERIMENT.SEED = args.seed

    if args.test:
        _cfg.EXPERIMENT.OUT_DIR = 'test-output'
        if args.skip_val:
            _cfg.EXPERIMENT.OUT_DIR = 'test-output-skip-val'
            _cfg.EXPERIMENT.MERGE_VAL_DATA_FOR_TEST = False
        _cfg.EXPERIMENT.VALIDATION_MODE = False

        if args.params is not None:
            if args.params != 'random':
                _cfg.EXPERIMENT.PARAMS = args.params
        else:
            # load from saved best hyperparameter
            _cfg.EXPERIMENT.PARAMS = f"""params/malware-detection/{args.dataset}_{args.dupes}/{args.model}/model_params.pkl"""

    MalwareDetection.run(_cfg)


if __name__ == '__main__':
    run()
